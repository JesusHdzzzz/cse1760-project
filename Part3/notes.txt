## Planned GBM Experiments (Hyperparameter Grid Ideas) - running on 3rd code cell

Where it says "# ---- Manual GBM variant (gbm_variant_1) ----"

| ID | Name                         | ntrees | max_depth | learn_rate | sample_rate | col_sample_rate | min_rows | Notes |
|----|------------------------------|--------|-----------|-----------:|------------:|----------------:|---------:|-------|
| 1  | Baseline                     | 100    | 5         | 0.10       | 1.0         | 1.0             | 10       | Simple starting point (already ran). |
| 2  | Shallow, many trees          | 500    | 4         | 0.05       | 1.0         | 1.0             | 10       | Tests underfitting vs stability. |
| 3  | Deep, fewer trees            | 200    | 10        | 0.05       | 1.0         | 1.0             | 5        | More complex trees, checks overfitting. |
| 4  | Deep, many trees             | 800    | 10        | 0.03       | 1.0         | 1.0             | 5        | Very strong learner, needs early stopping. |
| 5  | Very deep, stronger regular. | 800    | 12        | 0.03       | 0.8         | 0.8             | 10       | High-capacity with subsampling + min_rows. |
| 6  | Small LR, many trees         | 1000   | 8         | 0.01       | 0.9         | 0.9             | 5        | Slow learner; good test of `learn_rate`. |
| 7  | Higher LR, fewer trees       | 200    | 8         | 0.10       | 0.8         | 0.8             | 10       | Aggressive learning, see if it overshoots. |
| 8  | Row subsampling focus        | 500    | 8         | 0.05       | 0.7         | 1.0             | 5        | Tests effect of `sample_rate` only. |
| 9  | Column subsampling focus     | 500    | 8         | 0.05       | 1.0         | 0.7             | 5        | Tests effect of `col_sample_rate` only. |
| 10 | Strong subsampling both      | 800    | 8         | 0.05       | 0.7         | 0.7             | 5        | Max diversity; usually good for tabular data. |
| 11 | Regularized via min_rows     | 500    | 8         | 0.05       | 0.9         | 0.9             | 20       | Forces leaves to have more data, smoother model. |
| 12 | Compact model (fast)         | 150    | 6         | 0.07       | 0.9         | 0.9             | 10       | “Good enough” smaller model for speed. |



=== Best Tuned GBM (Grid Search): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 85.07859140163656
RMSE: 9.223805689715963
MAE: 5.0203285657367065
RMSLE: 0.377566035888705
Mean Residual Deviance: 85.07859140163656

=== Best Tuned GBM (Grid Search): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 91.10945400762134
RMSE: 9.54512723894351
MAE: 5.194156650961609
RMSLE: 0.3816283547466119
Mean Residual Deviance: 91.10945400762134

1. Baseline

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 114.35113479516781
RMSE: 10.693509002903014
MAE: 6.8773736310886955
RMSLE: NaN
Mean Residual Deviance: 114.35113479516781

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 130.7661342917119
RMSE: 11.43530210758386
MAE: 7.329962215328263
RMSLE: NaN
Mean Residual Deviance: 130.7661342917119

2. Shallow, many trees

ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 110.65503294120147
RMSE: 10.519269601127327
MAE: 6.682695503827137
RMSLE: NaN
Mean Residual Deviance: 110.65503294120147

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 126.56644504980714
RMSE: 11.25017533418067
MAE: 7.073546839945515
RMSLE: NaN
Mean Residual Deviance: 126.56644504980714

3. Deep, fewer trees

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 91.7882062585969
RMSE: 9.580616173221683
MAE: 5.328509737845169
RMSLE: NaN
Mean Residual Deviance: 91.7882062585969

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 95.98702883819506
RMSE: 9.797297016942737
MAE: 5.514143491953848
RMSLE: 0.40074237469548046
Mean Residual Deviance: 95.98702883819506

4. Deep, many trees

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 91.93809257471189
RMSE: 9.588435355922877
MAE: 5.231764815148115
RMSLE: NaN
Mean Residual Deviance: 91.93809257471189

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 93.48002688275191
RMSE: 9.66850696244006
MAE: 5.345524731556652
RMSLE: NaN
Mean Residual Deviance: 93.48002688275191

5. Very deep, stronger regular (BEST)

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 84.76604186830303
RMSE: 9.206847553223797
MAE: 4.98812160520563
RMSLE: 0.3815474224146949
Mean Residual Deviance: 84.76604186830303

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 90.93101501261914
RMSE: 9.535775532835236 (BEST results, before Standardized Features and Gaussianized Transformation)
MAE: 5.191092455933854
RMSLE: 0.3832104753363808
Mean Residual Deviance: 90.93101501261914

6. Small LR, many trees

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 90.4292185816191
RMSE: 9.509427878774785
MAE: 5.592691193791021
RMSLE: 0.428439634169597
Mean Residual Deviance: 90.4292185816191

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 97.79870034214616
RMSE: 9.889322542123205
MAE: 5.826224735140605
RMSLE: 0.42946848795981596
Mean Residual Deviance: 97.79870034214616

7. Higher LR, fewer trees

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 91.49473188605006
RMSE: 9.565287862163379
MAE: 5.644191763228555
RMSLE: NaN
Mean Residual Deviance: 91.49473188605006

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 98.5344615930079
RMSE: 9.926452618786225
MAE: 5.84946965348969
RMSLE: NaN
Mean Residual Deviance: 98.5344615930079

8. Row subsampling focus

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 88.72243456738013
RMSE: 9.419258705831375
MAE: 5.352213598185788
RMSLE: NaN
Mean Residual Deviance: 88.72243456738013

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 93.25689783158317
RMSE: 9.65696110749045
MAE: 5.504111165386398
RMSLE: 0.41372027970492276
Mean Residual Deviance: 93.25689783158317

9. Column subsampling focus

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 86.92616556434189
RMSE: 9.323420271785558
MAE: 5.222630328150416
RMSLE: 0.3998071394263899
Mean Residual Deviance: 86.92616556434189

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 93.66634787728104
RMSE: 9.678137624423464
MAE: 5.440744730538781
RMSLE: 0.395810289431508
Mean Residual Deviance: 93.66634787728104

10. Strong subsampling both

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 86.22803292746424
RMSE: 9.285905067760721
MAE: 5.170443614389118
RMSLE: NaN
Mean Residual Deviance: 86.22803292746424

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 91.52831535029598
RMSE: 9.567043187437589
MAE: 5.366661447314896
RMSLE: NaN
Mean Residual Deviance: 91.52831535029598

11. Regularized via min_rows

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 89.75114355328897
RMSE: 9.47370801499017
MAE: 5.508592454233012
RMSLE: NaN
Mean Residual Deviance: 89.75114355328897

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 96.85185283572888
RMSE: 9.84133389514495
MAE: 5.7233396949748165
RMSLE: 0.4223308596827402
Mean Residual Deviance: 96.85185283572888

12. Compact model (fast)

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 103.0828839281498
RMSE: 10.152974142001437
MAE: 6.3030748093834665
RMSLE: NaN
Mean Residual Deviance: 103.0828839281498

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 116.89347745165155
RMSE: 10.811728698577834
MAE: 6.703816220792203
RMSLE: NaN
Mean Residual Deviance: 116.89347745165155

13. Standarized Features

=== GBM (Standardized Features): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 85.34755139036145
RMSE: 9.238373849891627
MAE: 5.014173635886805
RMSLE: NaN
Mean Residual Deviance: 85.34755139036145

=== GBM (Standardized Features): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 90.3572327125182
RMSE: 9.505642151507608
MAE: 5.186564187530019
RMSLE: NaN
Mean Residual Deviance: 90.3572327125182

14. "Gaussianized" Features (Log Transform)

=== GBM (Gaussianized Features): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 85.13324971921752
RMSE: 9.226768108022306
MAE: 5.007806709648424
RMSLE: 0.3777978528924478
Mean Residual Deviance: 85.13324971921752

=== GBM (Gaussianized Features): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 90.59510933841406
RMSE: 9.518146318396983
MAE: 5.151140238331981
RMSLE: NaN
Mean Residual Deviance: 90.59510933841406

15. Dimensionality Reduction + GBM

## PCA-based GBM: project features, then train GBM on PCs

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Convert H2OFrames to pandas
train_pd = train.as_data_frame()
valid_pd = valid.as_data_frame()
test_pd  = test.as_data_frame()

y = "critical_temp"
X_cols = [c for c in train_pd.columns if c != y]

X_train_np = train_pd[X_cols].values
X_valid_np = valid_pd[X_cols].values
X_test_np  = test_pd[X_cols].values

y_train_np = train_pd[y].values
y_valid_np = valid_pd[y].values
y_test_np  = test_pd[y].values

# Standardize before PCA
scaler_pca = StandardScaler()
X_train_scaled = scaler_pca.fit_transform(X_train_np)
X_valid_scaled = scaler_pca.transform(X_valid_np)
X_test_scaled  = scaler_pca.transform(X_test_np)

# PCA: keep 95% variance
pca = PCA(n_components=0.95, random_state=176)
X_train_pca = pca.fit_transform(X_train_scaled)
X_valid_pca = pca.transform(X_valid_scaled)
X_test_pca  = pca.transform(X_test_scaled)

print("Original dim:", X_train_scaled.shape[1])
print("PCA dim:", X_train_pca.shape[1])

# Build pandas DataFrames for PCA features + target
pc_names = [f"PC{i+1}" for i in range(X_train_pca.shape[1])]

train_pca_pd = pd.DataFrame(X_train_pca, columns=pc_names)
valid_pca_pd = pd.DataFrame(X_valid_pca, columns=pc_names)
test_pca_pd  = pd.DataFrame(X_test_pca,  columns=pc_names)

train_pca_pd[y] = y_train_np
valid_pca_pd[y] = y_valid_np
test_pca_pd[y]  = y_test_np

# Convert to H2OFrames
train_pca = h2o.H2OFrame(train_pca_pd)
valid_pca = h2o.H2OFrame(valid_pca_pd)
test_pca  = h2o.H2OFrame(test_pca_pd)

# Train GBM on PCA features using your strong config
from h2o.estimators import H2OGradientBoostingEstimator

gbm_pca = H2OGradientBoostingEstimator(
    ntrees=800,
    max_depth=12,
    learn_rate=0.03,
    sample_rate=0.8,
    col_sample_rate=0.8,
    min_rows=10,
    stopping_rounds=5,
    stopping_metric="RMSE",
    stopping_tolerance=1e-4,
    seed=176
)

gbm_pca.train(
    x=pc_names,
    y=y,
    training_frame=train_pca,
    validation_frame=valid_pca
)

print("=== GBM PCA: Validation Performance ===")
perf_valid_pca = gbm_pca.model_performance(valid_pca)
perf_valid_pca.show()

print("\n=== GBM PCA: Test Performance ===")
perf_test_pca = gbm_pca.model_performance(test_pca)
perf_test_pca.show()

16. Feature Selection (Top-K by Mutual Information) + GBM

=== GBM Top-K: Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 0.017075552379526553
RMSE: 0.13067345705814382
MAE: 0.04201109492218169
RMSLE: 0.014284448715796453
Mean Residual Deviance: 0.017075552379526553

=== GBM Top-K: Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 0.01401085402685256
RMSE: 0.118367453410355
MAE: 0.03950371643147975
RMSLE: 0.006176284407183656
Mean Residual Deviance: 0.01401085402685256

17. Outlier Removal on Target + GBM

=== GBM (No Outliers): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 0.015961915027374297
RMSE: 0.12634047264188264
MAE: 0.03663568792197648
RMSLE: 0.010993928459842267
Mean Residual Deviance: 0.015961915027374297

=== GBM (No Outliers): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 0.040072989089466854
RMSE: 0.20018238955878925
MAE: 0.04001260170198679
RMSLE: 0.008050600615962404
Mean Residual Deviance: 0.040072989089466854

Summary from supercon_full_experiments.py

===== Building final summary table =====
                                         experiment  valid_RMSE  valid_MAE  test_RMSE  test_MAE  train_time_sec  ntrees  max_depth  learn_rate  sample_rate  col_sample_rate  min_rows
0                          GBM (No Target Outliers)    9.232673   5.010276   9.387624  5.124456       21.771128     800         12        0.03          0.8              0.8        10
1                       GBM (Standardized Features)    9.238374   5.014174   9.505642  5.186564       16.504978     800         12        0.03          0.8              0.8        10
2                       GBM (Gaussianized Features)    9.226768   5.007807   9.518146  5.151140       21.015670     800         12        0.03          0.8              0.8        10
3    GBM Manual 5: Very deep, strong regular (ID 5)    9.206848   4.988122   9.535776  5.191092       22.236318     800         12        0.03          0.8              0.8        10
4    GBM Manual 10: Strong subsampling both (ID 10)    9.285905   5.170444   9.567043  5.366661       10.992856     800          8        0.05          0.7              0.7         5
5        GBM Manual 8: Row subsampling focus (ID 8)    9.419259   5.352214   9.656961  5.504111        8.709814     500          8        0.05          0.7              1.0         5
6             GBM Manual 4: Deep, many trees (ID 4)    9.588435   5.231765   9.668507  5.345525       19.392201     800         10        0.03          1.0              1.0         5
7     GBM Manual 9: Column subsampling focus (ID 9)    9.323420   5.222630   9.678138  5.440745        8.144436     500          8        0.05          1.0              0.7         5
8            GBM Manual 3: Deep, fewer trees (ID 3)    9.580616   5.328510   9.797297  5.514143        9.015990     200         10        0.05          1.0              1.0         5
9   GBM Manual 11: Regularized via min_rows (ID 11)    9.473708   5.508592   9.841334  5.723340        7.251198     500          8        0.05          0.9              0.9        20
10        GBM Manual 6: Small LR, many trees (ID 6)    9.509428   5.592691   9.889323  5.826225       16.605163    1000          8        0.01          0.9              0.9         5
11      GBM Manual 7: Higher LR, fewer trees (ID 7)    9.565288   5.644192   9.926453  5.849470        3.690577     200          8        0.10          0.8              0.8        10
12                           GBM (PCA 95% variance)    9.761743   5.415984  10.073292  5.560451        9.374571     800         12        0.03          0.8              0.8        10
13                         GBM (Top-20 MI Features)   10.161546   5.938719  10.639558  6.207212        3.074209     800         12        0.03          0.8              0.8        10
14      GBM Manual 12: Compact model (fast) (ID 12)   10.152974   6.303075  10.811729  6.703816        1.779909     150          6        0.07          0.9              0.9        10
15         GBM Manual 2: Shallow, many trees (ID 2)   10.519270   6.682696  11.250175  7.073547        7.825186     500          4        0.05          1.0              1.0        10
16                              GBM Baseline (ID 1)   10.693509   6.877374  11.435302  7.329962        1.789592     100          5        0.10          1.0              1.0        10