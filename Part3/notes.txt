## Planned GBM Experiments (Hyperparameter Grid Ideas) - running on 3rd code cell

Where it says "# ---- Manual GBM variant (gbm_variant_1) ----"

| ID | Name                         | ntrees | max_depth | learn_rate | sample_rate | col_sample_rate | min_rows | Notes |
|----|------------------------------|--------|-----------|-----------:|------------:|----------------:|---------:|-------|
| 1  | Baseline                     | 100    | 5         | 0.10       | 1.0         | 1.0             | 10       | Simple starting point (already ran). |
| 2  | Shallow, many trees          | 500    | 4         | 0.05       | 1.0         | 1.0             | 10       | Tests underfitting vs stability. |
| 3  | Deep, fewer trees            | 200    | 10        | 0.05       | 1.0         | 1.0             | 5        | More complex trees, checks overfitting. |
| 4  | Deep, many trees             | 800    | 10        | 0.03       | 1.0         | 1.0             | 5        | Very strong learner, needs early stopping. |
| 5  | Very deep, stronger regular. | 800    | 12        | 0.03       | 0.8         | 0.8             | 10       | High-capacity with subsampling + min_rows. |
| 6  | Small LR, many trees         | 1000   | 8         | 0.01       | 0.9         | 0.9             | 5        | Slow learner; good test of `learn_rate`. |
| 7  | Higher LR, fewer trees       | 200    | 8         | 0.10       | 0.8         | 0.8             | 10       | Aggressive learning, see if it overshoots. |
| 8  | Row subsampling focus        | 500    | 8         | 0.05       | 0.7         | 1.0             | 5        | Tests effect of `sample_rate` only. |
| 9  | Column subsampling focus     | 500    | 8         | 0.05       | 1.0         | 0.7             | 5        | Tests effect of `col_sample_rate` only. |
| 10 | Strong subsampling both      | 800    | 8         | 0.05       | 0.7         | 0.7             | 5        | Max diversity; usually good for tabular data. |
| 11 | Regularized via min_rows     | 500    | 8         | 0.05       | 0.9         | 0.9             | 20       | Forces leaves to have more data, smoother model. |
| 12 | Compact model (fast)         | 150    | 6         | 0.07       | 0.9         | 0.9             | 10       | “Good enough” smaller model for speed. |



=== Best Tuned GBM (Grid Search): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 85.07859140163656
RMSE: 9.223805689715963
MAE: 5.0203285657367065
RMSLE: 0.377566035888705
Mean Residual Deviance: 85.07859140163656

=== Best Tuned GBM (Grid Search): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 91.10945400762134
RMSE: 9.54512723894351
MAE: 5.194156650961609
RMSLE: 0.3816283547466119
Mean Residual Deviance: 91.10945400762134

1. Baseline

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 114.35113479516781
RMSE: 10.693509002903014
MAE: 6.8773736310886955
RMSLE: NaN
Mean Residual Deviance: 114.35113479516781

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 130.7661342917119
RMSE: 11.43530210758386
MAE: 7.329962215328263
RMSLE: NaN
Mean Residual Deviance: 130.7661342917119

2. Shallow, many trees

ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 110.65503294120147
RMSE: 10.519269601127327
MAE: 6.682695503827137
RMSLE: NaN
Mean Residual Deviance: 110.65503294120147

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 126.56644504980714
RMSE: 11.25017533418067
MAE: 7.073546839945515
RMSLE: NaN
Mean Residual Deviance: 126.56644504980714

3. Deep, fewer trees

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 91.7882062585969
RMSE: 9.580616173221683
MAE: 5.328509737845169
RMSLE: NaN
Mean Residual Deviance: 91.7882062585969

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 95.98702883819506
RMSE: 9.797297016942737
MAE: 5.514143491953848
RMSLE: 0.40074237469548046
Mean Residual Deviance: 95.98702883819506

4. Deep, many trees

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 91.93809257471189
RMSE: 9.588435355922877
MAE: 5.231764815148115
RMSLE: NaN
Mean Residual Deviance: 91.93809257471189

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 93.48002688275191
RMSE: 9.66850696244006
MAE: 5.345524731556652
RMSLE: NaN
Mean Residual Deviance: 93.48002688275191

5. Very deep, stronger regular (BEST)

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 84.76604186830303
RMSE: 9.206847553223797
MAE: 4.98812160520563
RMSLE: 0.3815474224146949
Mean Residual Deviance: 84.76604186830303

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 90.93101501261914
RMSE: 9.535775532835236 (BEST results, before Standardized Features and Gaussianized Transformation)
MAE: 5.191092455933854
RMSLE: 0.3832104753363808
Mean Residual Deviance: 90.93101501261914

6. Small LR, many trees

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 90.4292185816191
RMSE: 9.509427878774785
MAE: 5.592691193791021
RMSLE: 0.428439634169597
Mean Residual Deviance: 90.4292185816191

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 97.79870034214616
RMSE: 9.889322542123205
MAE: 5.826224735140605
RMSLE: 0.42946848795981596
Mean Residual Deviance: 97.79870034214616

7. Higher LR, fewer trees

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 91.49473188605006
RMSE: 9.565287862163379
MAE: 5.644191763228555
RMSLE: NaN
Mean Residual Deviance: 91.49473188605006

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 98.5344615930079
RMSE: 9.926452618786225
MAE: 5.84946965348969
RMSLE: NaN
Mean Residual Deviance: 98.5344615930079

8. Row subsampling focus

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 88.72243456738013
RMSE: 9.419258705831375
MAE: 5.352213598185788
RMSLE: NaN
Mean Residual Deviance: 88.72243456738013

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 93.25689783158317
RMSE: 9.65696110749045
MAE: 5.504111165386398
RMSLE: 0.41372027970492276
Mean Residual Deviance: 93.25689783158317

9. Column subsampling focus

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 86.92616556434189
RMSE: 9.323420271785558
MAE: 5.222630328150416
RMSLE: 0.3998071394263899
Mean Residual Deviance: 86.92616556434189

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 93.66634787728104
RMSE: 9.678137624423464
MAE: 5.440744730538781
RMSLE: 0.395810289431508
Mean Residual Deviance: 93.66634787728104

10. Strong subsampling both

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 86.22803292746424
RMSE: 9.285905067760721
MAE: 5.170443614389118
RMSLE: NaN
Mean Residual Deviance: 86.22803292746424

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 91.52831535029598
RMSE: 9.567043187437589
MAE: 5.366661447314896
RMSLE: NaN
Mean Residual Deviance: 91.52831535029598

11. Regularized via min_rows

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 89.75114355328897
RMSE: 9.47370801499017
MAE: 5.508592454233012
RMSLE: NaN
Mean Residual Deviance: 89.75114355328897

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 96.85185283572888
RMSE: 9.84133389514495
MAE: 5.7233396949748165
RMSLE: 0.4223308596827402
Mean Residual Deviance: 96.85185283572888

12. Compact model (fast)

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 103.0828839281498
RMSE: 10.152974142001437
MAE: 6.3030748093834665
RMSLE: NaN
Mean Residual Deviance: 103.0828839281498

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 116.89347745165155
RMSE: 10.811728698577834
MAE: 6.703816220792203
RMSLE: NaN
Mean Residual Deviance: 116.89347745165155

13. Standarized Features

=== GBM (Standardized Features): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 85.34755139036145
RMSE: 9.238373849891627
MAE: 5.014173635886805
RMSLE: NaN
Mean Residual Deviance: 85.34755139036145

=== GBM (Standardized Features): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 90.3572327125182
RMSE: 9.505642151507608
MAE: 5.186564187530019
RMSLE: NaN
Mean Residual Deviance: 90.3572327125182

14. "Gaussianized" Features (Log Transform)

=== GBM (Gaussianized Features): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 85.13324971921752
RMSE: 9.226768108022306
MAE: 5.007806709648424
RMSLE: 0.3777978528924478
Mean Residual Deviance: 85.13324971921752

=== GBM (Gaussianized Features): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 90.59510933841406
RMSE: 9.518146318396983
MAE: 5.151140238331981
RMSLE: NaN
Mean Residual Deviance: 90.59510933841406

15. Dimensionality Reduction + GBM



16. Feature Selection (Top-K by Mutual Information) + GBM

=== GBM Top-K: Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 0.017075552379526553
RMSE: 0.13067345705814382
MAE: 0.04201109492218169
RMSLE: 0.014284448715796453
Mean Residual Deviance: 0.017075552379526553

=== GBM Top-K: Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 0.01401085402685256
RMSE: 0.118367453410355
MAE: 0.03950371643147975
RMSLE: 0.006176284407183656
Mean Residual Deviance: 0.01401085402685256

17. Outlier Removal on Target + GBM

=== GBM (No Outliers): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 0.015961915027374297
RMSE: 0.12634047264188264
MAE: 0.03663568792197648
RMSLE: 0.010993928459842267
Mean Residual Deviance: 0.015961915027374297

=== GBM (No Outliers): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 0.040072989089466854
RMSE: 0.20018238955878925
MAE: 0.04001260170198679
RMSLE: 0.008050600615962404
Mean Residual Deviance: 0.040072989089466854

Summary from supercon_full_experiments.py

===== Building final summary table =====
                                         experiment  valid_RMSE  valid_MAE  test_RMSE  test_MAE  train_time_sec  ntrees  max_depth  learn_rate  sample_rate  col_sample_rate  min_rows
0                          GBM (No Target Outliers)    9.232673   5.010276   9.387624  5.124456       21.771128     800         12        0.03          0.8              0.8        10
1                       GBM (Standardized Features)    9.238374   5.014174   9.505642  5.186564       16.504978     800         12        0.03          0.8              0.8        10
2                       GBM (Gaussianized Features)    9.226768   5.007807   9.518146  5.151140       21.015670     800         12        0.03          0.8              0.8        10
3    GBM Manual 5: Very deep, strong regular (ID 5)    9.206848   4.988122   9.535776  5.191092       22.236318     800         12        0.03          0.8              0.8        10
4    GBM Manual 10: Strong subsampling both (ID 10)    9.285905   5.170444   9.567043  5.366661       10.992856     800          8        0.05          0.7              0.7         5
5        GBM Manual 8: Row subsampling focus (ID 8)    9.419259   5.352214   9.656961  5.504111        8.709814     500          8        0.05          0.7              1.0         5
6             GBM Manual 4: Deep, many trees (ID 4)    9.588435   5.231765   9.668507  5.345525       19.392201     800         10        0.03          1.0              1.0         5
7     GBM Manual 9: Column subsampling focus (ID 9)    9.323420   5.222630   9.678138  5.440745        8.144436     500          8        0.05          1.0              0.7         5
8            GBM Manual 3: Deep, fewer trees (ID 3)    9.580616   5.328510   9.797297  5.514143        9.015990     200         10        0.05          1.0              1.0         5
9   GBM Manual 11: Regularized via min_rows (ID 11)    9.473708   5.508592   9.841334  5.723340        7.251198     500          8        0.05          0.9              0.9        20
10        GBM Manual 6: Small LR, many trees (ID 6)    9.509428   5.592691   9.889323  5.826225       16.605163    1000          8        0.01          0.9              0.9         5
11      GBM Manual 7: Higher LR, fewer trees (ID 7)    9.565288   5.644192   9.926453  5.849470        3.690577     200          8        0.10          0.8              0.8        10
12                           GBM (PCA 95% variance)    9.761743   5.415984  10.073292  5.560451        9.374571     800         12        0.03          0.8              0.8        10
13                         GBM (Top-20 MI Features)   10.161546   5.938719  10.639558  6.207212        3.074209     800         12        0.03          0.8              0.8        10
14      GBM Manual 12: Compact model (fast) (ID 12)   10.152974   6.303075  10.811729  6.703816        1.779909     150          6        0.07          0.9              0.9        10
15         GBM Manual 2: Shallow, many trees (ID 2)   10.519270   6.682696  11.250175  7.073547        7.825186     500          4        0.05          1.0              1.0        10
16                              GBM Baseline (ID 1)   10.693509   6.877374  11.435302  7.329962        1.789592     100          5        0.10          1.0              1.0        10

Summary from run_superconductivity_models.py

===== Model Comparison Summary =====
         experiment_name        cv_RMSE   cv_MAE   test_RMSE   test_MAE   train_time_sec   ntrees   max_depth   learn_rate   sample_rate   col_sample_rate   min_rows   n_params
0        H2O GBM (Full)          9.4183   5.3780      8.8289     5.1251          40.7403    500.0        8.0        0.05          0.8              0.8         5.0      48823
1   H2O Random Forest             9.7518   5.6236      9.2194     5.3503         217.0345    500.0        20.0        N/A           0.8              0.8         5.0     963397
2      SVR (RBF + PCA 50)        14.9443   9.1151     14.4189     8.7580          22.2509      N/A         N/A        N/A            N/A               N/A         N/A     828240
3        H2O GLM (Linear)        17.8707  13.5830     17.6749    13.4082           2.6023      N/A         N/A        N/A            N/A               N/A         N/A         82

Summary from new run_superconductivity_models.py version

====================== MODEL COMPARISON (Sorted by Test RMSE) ======================
                        model_name              algorithm    cv_rmse  test_rmse     cv_mae   test_mae  n_params  n_trees  train_time_sec                                notes
0   GBM_VeryDeep_StrongRegular_ID5              GBM_sweep   9.387606   8.722747   5.194025   4.869640    154356    800.0       87.513724                                     
1                          H2O_GBM                    GBM   9.418285   8.828913   5.378000   5.125130     48823    500.0       40.762553                                     
2       GBM_StrongSubsampling_ID10              GBM_sweep   9.397730   8.829993   5.309198   5.069591     73956    800.0       54.683158                                     
3           GBM_ColSubsampling_ID9              GBM_sweep   9.437313   8.884047   5.375641   5.156845     47585    500.0       40.910616                                     
4           GBM_Deep_ManyTrees_ID4              GBM_sweep   9.556122   8.896479   5.319660   5.091122    104625    560.0       65.582003                                     
5          GBM_Deep_FewerTrees_ID3              GBM_sweep   9.630900   8.930458   5.452263   5.197162     45164    200.0       30.189330                                     
6           GBM_RowSubsampling_ID8              GBM_sweep   9.470983   8.958749   5.423870   5.207333     46174    500.0       43.114455                                     
7        GBM_HighLR_FewerTrees_ID7              GBM_sweep   9.668325   9.092212   5.670200   5.432023     15666    200.0       20.486852                                     
8             GBM_Reg_MinRows_ID11              GBM_sweep   9.628454   9.105844   5.621602   5.410557     30073    500.0       40.836224                                     
9        GBM_SmallLR_ManyTrees_ID6              GBM_sweep   9.612415   9.136206   5.700515   5.534838    117849   1000.0       82.172448                                     
10                          H2O_RF           RandomForest   9.751779   9.219407   5.623556   5.350259    963397    500.0      219.656855  very_large_model,very_slow_training
11           GBM_Compact_Fast_ID12              GBM_sweep  10.363667  10.107317   6.475930   6.413636      6240    150.0       14.035823                                     
12       GBM_Shallow_ManyTrees_ID2              GBM_sweep  10.787570  10.619564   6.854068   6.853285      7317    500.0       25.651416                                     
13                GBM_Baseline_ID1              GBM_sweep  10.931915  10.730998   7.044464   7.050939      2507    100.0        7.970552                                     
14                         SVR_PCA           SVR(RBF)+PCA  14.944278  14.418936   9.115104   8.758011    828240      NaN       25.169873     very_high_error,very_large_model
15                         H2O_GLM  LinearRegression(GLM)  17.870723  17.674896  13.582972  13.408205        82      NaN        3.135192                      very_high_error
====================================================================================

Notes about these results ^^:
- GLM performed the worse and ran into a "max iterations 31!" warning during training, which makes sense since it's a 
linear model looking for a solution in a non-linear problem.
- Random Forest had way too many features, almost a million, that's around 20x more than the base GBM model and it was 5x 
slower as well. This is a clear example that more trees doesn't necessarily mean better results. It most likely overfitted 
during training.
- SVR with PCA, performed worse than GBM and RF. Probably doesn't work very well with such high dimensionality from the dataset. 
Since our dataset contains over 21,000 samples and the complexity is quadratic, it reaches a point where there's too much data to be saved
in RAM. Overall, our dataset is so big SVR (RBF Kernel) struggles fitting the data cleanly. 

I will run ID5 "GBM_VeryDeep_StrongRegular" again and tweak Hyperparameters to see if I can get faster and better results.

===== Building final summary table (GBM base + sweep) =====
      experiment  valid_RMSE  valid_MAE  test_rmse  test_mae  train_time_sec  n_trees  max_depth  learn_rate  sample_rate  col_sample_rate  min_rows  n_params
0   GBM_n800_d11    9.353175   5.225667   8.701858  4.911562       72.801971      800       11.0        0.03          0.8              0.8        10    124306
1   GBM_n700_d11    9.357677   5.247294   8.711788  4.941400       67.290985      700       11.0        0.03          0.8              0.8        10    110638
2   GBM_n800_d10    9.349486   5.259132   8.714611  5.000508       66.681718      800       10.0        0.03          0.8              0.8        10    101444
3   GBM_n800_d12    9.387606   5.194025   8.722747  4.869640       82.262732      800       12.0        0.03          0.8              0.8        10    154356
4   GBM_n700_d12    9.384953   5.211309   8.726258  4.892069       74.699350      700       12.0        0.03          0.8              0.8        10    137016
5   GBM_n600_d12    9.387363   5.230574   8.735288  4.918195       69.105739      600       12.0        0.03          0.8              0.8        10    120514
6   GBM_n600_d11    9.365266   5.271230   8.738095  4.978409       64.128143      600       11.0        0.03          0.8              0.8        10     97743
7   GBM_n700_d10    9.367268   5.294229   8.741757  5.042474       62.718960      700       10.0        0.03          0.8              0.8        10     89892
8   GBM_n500_d12    9.396070   5.257332   8.762345  4.956132       64.666366      500       12.0        0.03          0.8              0.8        10    105485
9   GBM_n600_d10    9.388291   5.333194   8.769550  5.091805       57.653268      600       10.0        0.03          0.8              0.8        10     78877
10  GBM_n500_d11    9.382214   5.301326   8.780153  5.021293       57.616376      500       11.0        0.03          0.8              0.8        10     85800
11   GBM_n800_d9    9.390258   5.340266   8.805403  5.100737       59.456543      800        9.0        0.03          0.8              0.8        10     78949
12  GBM_n500_d10    9.416965   5.377632   8.825882  5.157239       51.011491      500       10.0        0.03          0.8              0.8        10     68390
13  H2O_GBM_Base    9.418285   5.378000   8.828913  5.125130       39.398304      500        8.0        0.05          0.8              0.8         5     48823
14   GBM_n700_d9    9.415660   5.384876   8.841752  5.159305       56.973970      700        9.0        0.03          0.8              0.8        10     69266
15   GBM_n600_d9    9.456359   5.440260   8.887069  5.218295       52.073334      600        9.0        0.03          0.8              0.8        10     60531
16   GBM_n500_d9    9.496477   5.499711   8.967154  5.298316       45.476267      500        9.0        0.03          0.8              0.8        10     52937
==========================================================

Notes ^^:
- GBM with 800 trees and depth of 11 seems to be the best model I've run so far compared to the other ones. I will try to experiment
with other Hyperparameters, manipulating dataset, and/or stopping early to see if we can get a more accurate model.

===== Building final summary table (GBM base + sweep) EARLY STOPPING =====
      experiment  valid_RMSE  valid_MAE  test_rmse  test_mae  train_time_sec  n_trees  max_depth  learn_rate  sample_rate  col_sample_rate  min_rows  n_params
0   GBM_n800_d11    9.372019   5.285202   8.757217  4.995664       55.421425      551       11.0        0.03          0.8              0.8        10     91738
1   GBM_n700_d11    9.372019   5.285202   8.757217  4.995664       58.238215      551       11.0        0.03          0.8              0.8        10     91738
2   GBM_n800_d10    9.375246   5.310766   8.757401  5.071927       56.114863      642       10.0        0.03          0.8              0.8        10     83374
3   GBM_n700_d10    9.376835   5.315238   8.763901  5.080841       57.453958      630       10.0        0.03          0.8              0.8        10     82011
4   GBM_n600_d11    9.374421   5.288776   8.764158  5.004190       57.446693      539       11.0        0.03          0.8              0.8        10     90264
5   GBM_n700_d12    9.388606   5.257996   8.766787  4.961228       59.810644      490       12.0        0.03          0.8              0.8        10    104046
6   GBM_n800_d12    9.388606   5.257996   8.766787  4.961228       58.084919      490       12.0        0.03          0.8              0.8        10    104046
7   GBM_n600_d12    9.390412   5.260478   8.771406  4.965062       62.436094      479       12.0        0.03          0.8              0.8        10    102413
8   GBM_n500_d12    9.395663   5.266390   8.778366  4.973413       60.075531      459       12.0        0.03          0.8              0.8        10     99221
9   GBM_n500_d11    9.382214   5.301326   8.780153  5.021293       56.523228      500       11.0        0.03          0.8              0.8        10     85800
10  GBM_n600_d10    9.390696   5.338634   8.783258  5.104414       54.993020      583       10.0        0.03          0.8              0.8        10     76862
11   GBM_n800_d9    9.397286   5.356023   8.821548  5.125503       57.644526      760        9.0        0.03          0.8              0.8        10     74999
12  GBM_n500_d10    9.416965   5.377632   8.825882  5.157239       51.186113      500       10.0        0.03          0.8              0.8        10     68390
13  H2O_GBM_Base    9.422826   5.401873   8.839749  5.157440       37.881779      458        8.0        0.05          0.8              0.8         5     45016
14   GBM_n700_d9    9.419321   5.393579   8.852707  5.170919       54.616793      680        9.0        0.03          0.8              0.8        10     67621
15   GBM_n600_d9    9.456359   5.440260   8.887069  5.218295       51.052400      600        9.0        0.03          0.8              0.8        10     60531
16   GBM_n500_d9    9.496477   5.499711   8.967154  5.298316       45.494767      500        9.0        0.03          0.8              0.8        10     52937
==========================================================

Notes from these results ^^:
- Despite stopping early the results still came back to be worse than when we let the model reach the max amount of trees.

These results below are from GBM (PCA 95% variance) in line 413.

===== PCA SUMMARY =====
Original features:  81
PCA components:     17

=== Variance Explained by Each PC ===
      PC  Explained_Variance_Ratio  Cumulative_Variance
0    PC1                  0.390461             0.390461
1    PC2                  0.104131             0.494592
2    PC3                  0.094710             0.589302
3    PC4                  0.078769             0.668071
4    PC5                  0.059314             0.727385
5    PC6                  0.037555             0.764941
6    PC7                  0.036096             0.801036
7    PC8                  0.031493             0.832529
8    PC9                  0.023678             0.856207
9   PC10                  0.019694             0.875901
10  PC11                  0.018397             0.894298
11  PC12                  0.014527             0.908825
12  PC13                  0.011807             0.920632
13  PC14                  0.009954             0.930586
14  PC15                  0.009685             0.940271
15  PC16                  0.007817             0.948088
16  PC17                  0.007175             0.955263

Total variance captured by PCA: 0.9553

=== Top contributing features for each PC ===

PC1:
  Top positive contributors:
    range_fie                            0.1637
    wtd_entropy_atomic_radius            0.1635
    range_atomic_radius                  0.1633
    wtd_std_fie                          0.1633
    wtd_std_atomic_radius                0.1602
  Top negative contributors:
    wtd_gmean_Density                   -0.1559
    gmean_Density                       -0.1506
    wtd_gmean_Valence                   -0.1466
    wtd_mean_Valence                    -0.1445
    gmean_Valence                       -0.1413

PC2:
  Top positive contributors:
    gmean_atomic_radius                  0.2376
    mean_atomic_radius                   0.2337
    mean_atomic_mass                     0.2268
    gmean_atomic_mass                    0.2210
    wtd_mean_atomic_mass                 0.1946
  Top negative contributors:
    std_FusionHeat                      -0.2128
    wtd_std_FusionHeat                  -0.2063
    range_FusionHeat                    -0.1957
    mean_fie                            -0.1841
    gmean_fie                           -0.1834

PC3:
  Top positive contributors:
    wtd_entropy_ThermalConductivity      0.2531
    wtd_std_Valence                      0.2280
    range_Valence                        0.2237
    entropy_ThermalConductivity          0.2150
    wtd_entropy_fie                      0.2135
  Top negative contributors:
    wtd_range_fie                       -0.1512
    std_ThermalConductivity             -0.1155
    wtd_std_ThermalConductivity         -0.1041
    range_ThermalConductivity           -0.0985
    mean_atomic_radius                  -0.0879

PC4:
  Top positive contributors:
    wtd_mean_ElectronAffinity            0.2752
    mean_ElectronAffinity                0.2720
    wtd_gmean_ElectronAffinity           0.2655
    gmean_ElectronAffinity               0.2324
    wtd_range_ElectronAffinity           0.1960
  Top negative contributors:
    entropy_ThermalConductivity         -0.1897
    wtd_entropy_ThermalConductivity     -0.1582
    range_FusionHeat                    -0.1169
    entropy_Density                     -0.1111
    std_FusionHeat                      -0.1067

PC5:
  Top positive contributors:
    wtd_mean_ThermalConductivity         0.2933
    mean_ThermalConductivity             0.2839
    wtd_range_ThermalConductivity        0.2593
    wtd_gmean_ThermalConductivity        0.2128
    wtd_range_FusionHeat                 0.1976
  Top negative contributors:
    wtd_std_Valence                     -0.1650
    range_Valence                       -0.1389
    std_Valence                         -0.1221
    gmean_fie                           -0.1050
    mean_fie                            -0.1024

PC6:
  Top positive contributors:
    wtd_range_fie                        0.2922
    wtd_range_ElectronAffinity           0.2497
    wtd_range_Valence                    0.2280
    wtd_gmean_ElectronAffinity           0.1887
    entropy_Density                      0.1696
  Top negative contributors:
    wtd_mean_ThermalConductivity        -0.2902
    wtd_range_ThermalConductivity       -0.2110
    gmean_fie                           -0.2085
    mean_fie                            -0.2025
    mean_ThermalConductivity            -0.1639

PC7:
  Top positive contributors:
    std_Density                          0.2862
    std_atomic_mass                      0.2658
    wtd_std_Density                      0.2540
    range_Density                        0.2533
    wtd_std_atomic_mass                  0.2481
  Top negative contributors:
    mean_ElectronAffinity               -0.2649
    gmean_ElectronAffinity              -0.2169
    wtd_mean_ElectronAffinity           -0.2095
    wtd_mean_ThermalConductivity        -0.2057
    wtd_range_ThermalConductivity       -0.1945

PC8:
  Top positive contributors:
    wtd_range_atomic_radius              0.3134
    wtd_range_Valence                    0.3033
    wtd_range_atomic_mass                0.2894
    wtd_range_Density                    0.2853
    std_Valence                          0.2827
  Top negative contributors:
    wtd_entropy_Density                 -0.1845
    gmean_FusionHeat                    -0.1786
    gmean_Valence                       -0.1490
    mean_FusionHeat                     -0.1468
    gmean_ElectronAffinity              -0.1444

PC9:
  Top positive contributors:
    std_Valence                          0.3255
    wtd_std_Valence                      0.2987
    range_Valence                        0.2917
    mean_ThermalConductivity             0.2515
    gmean_ThermalConductivity            0.2211
  Top negative contributors:
    wtd_range_atomic_radius             -0.2663
    wtd_range_atomic_mass               -0.2511
    wtd_range_Density                   -0.1977
    mean_fie                            -0.1721
    gmean_fie                           -0.1552

PC10:
  Top positive contributors:
    gmean_fie                            0.2688
    mean_ThermalConductivity             0.2359
    gmean_ThermalConductivity            0.2202
    mean_fie                             0.2116
    entropy_ThermalConductivity          0.2068
  Top negative contributors:
    mean_atomic_radius                  -0.2254
    std_atomic_radius                   -0.2230
    wtd_range_ThermalConductivity       -0.1592
    gmean_ElectronAffinity              -0.1507
    range_atomic_radius                 -0.1498

PC11:
  Top positive contributors:
    wtd_std_ElectronAffinity             0.3950
    std_ElectronAffinity                 0.3885
    range_ElectronAffinity               0.3184
    mean_atomic_radius                   0.1763
    wtd_range_ElectronAffinity           0.1627
  Top negative contributors:
    gmean_ElectronAffinity              -0.3211
    entropy_ElectronAffinity            -0.2619
    wtd_entropy_ElectronAffinity        -0.2451
    wtd_gmean_ElectronAffinity          -0.2235
    wtd_range_atomic_mass               -0.1540

PC12:
  Top positive contributors:
    mean_atomic_mass                     0.2822
    wtd_mean_atomic_mass                 0.2789
    gmean_atomic_mass                    0.2519
    wtd_gmean_atomic_mass                0.2420
    wtd_gmean_ThermalConductivity        0.1664
  Top negative contributors:
    gmean_Valence                       -0.2100
    wtd_range_atomic_radius             -0.1947
    mean_Valence                        -0.1919
    wtd_gmean_Valence                   -0.1861
    wtd_mean_atomic_radius              -0.1815

PC13:
  Top positive contributors:
    std_atomic_mass                      0.2821
    wtd_std_atomic_mass                  0.2561
    range_atomic_mass                    0.2297
    wtd_entropy_ThermalConductivity      0.2151
    std_ElectronAffinity                 0.1813
  Top negative contributors:
    std_Valence                         -0.2359
    gmean_fie                           -0.2162
    mean_fie                            -0.2137
    range_Valence                       -0.2117
    wtd_range_ThermalConductivity       -0.1942

PC14:
  Top positive contributors:
    std_ThermalConductivity              0.2858
    range_ThermalConductivity            0.2624
    range_FusionHeat                     0.2444
    std_FusionHeat                       0.2250
    wtd_std_ThermalConductivity          0.2157
  Top negative contributors:
    wtd_gmean_ThermalConductivity       -0.2652
    entropy_ThermalConductivity         -0.2299
    mean_atomic_radius                  -0.1957
    std_fie                             -0.1954
    gmean_ThermalConductivity           -0.1907

PC15:
  Top positive contributors:
    wtd_gmean_FusionHeat                 0.2820
    std_atomic_mass                      0.2694
    range_atomic_mass                    0.2606
    wtd_mean_FusionHeat                  0.2166
    wtd_range_FusionHeat                 0.2060
  Top negative contributors:
    range_FusionHeat                    -0.2653
    wtd_std_FusionHeat                  -0.2451
    std_FusionHeat                      -0.2245
    wtd_std_Density                     -0.2232
    std_Density                         -0.1985

PC16:
  Top positive contributors:
    gmean_FusionHeat                     0.3396
    mean_FusionHeat                      0.2578
    wtd_range_atomic_mass                0.2321
    mean_ThermalConductivity             0.2294
    std_ThermalConductivity              0.2143
  Top negative contributors:
    wtd_range_ThermalConductivity       -0.2671
    wtd_mean_ThermalConductivity        -0.1762
    wtd_gmean_ThermalConductivity       -0.1687
    entropy_ThermalConductivity         -0.1627
    range_Density                       -0.1553

PC17:
  Top positive contributors:
    wtd_range_Density                    0.3358
    wtd_range_ElectronAffinity           0.2034
    wtd_range_atomic_mass                0.1660
    wtd_mean_Density                     0.1505
    wtd_mean_ElectronAffinity            0.1466
  Top negative contributors:
    wtd_range_atomic_radius             -0.2879
    wtd_range_FusionHeat                -0.2332
    mean_ElectronAffinity               -0.2037
    gmean_ElectronAffinity              -0.1938
    mean_atomic_mass                    -0.1874