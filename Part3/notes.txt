## Planned GBM Experiments (Hyperparameter Grid Ideas) - running on 3rd code cell

Where it says "# ---- Manual GBM variant (gbm_variant_1) ----"

| ID | Name                         | ntrees | max_depth | learn_rate | sample_rate | col_sample_rate | min_rows | Notes |
|----|------------------------------|--------|-----------|-----------:|------------:|----------------:|---------:|-------|
| 1  | Baseline                     | 100    | 5         | 0.10       | 1.0         | 1.0             | 10       | Simple starting point (already ran). |
| 2  | Shallow, many trees          | 500    | 4         | 0.05       | 1.0         | 1.0             | 10       | Tests underfitting vs stability. |
| 3  | Deep, fewer trees            | 200    | 10        | 0.05       | 1.0         | 1.0             | 5        | More complex trees, checks overfitting. |
| 4  | Deep, many trees             | 800    | 10        | 0.03       | 1.0         | 1.0             | 5        | Very strong learner, needs early stopping. |
| 5  | Very deep, stronger regular. | 800    | 12        | 0.03       | 0.8         | 0.8             | 10       | High-capacity with subsampling + min_rows. |
| 6  | Small LR, many trees         | 1000   | 8         | 0.01       | 0.9         | 0.9             | 5        | Slow learner; good test of `learn_rate`. |
| 7  | Higher LR, fewer trees       | 200    | 8         | 0.10       | 0.8         | 0.8             | 10       | Aggressive learning, see if it overshoots. |
| 8  | Row subsampling focus        | 500    | 8         | 0.05       | 0.7         | 1.0             | 5        | Tests effect of `sample_rate` only. |
| 9  | Column subsampling focus     | 500    | 8         | 0.05       | 1.0         | 0.7             | 5        | Tests effect of `col_sample_rate` only. |
| 10 | Strong subsampling both      | 800    | 8         | 0.05       | 0.7         | 0.7             | 5        | Max diversity; usually good for tabular data. |
| 11 | Regularized via min_rows     | 500    | 8         | 0.05       | 0.9         | 0.9             | 20       | Forces leaves to have more data, smoother model. |
| 12 | Compact model (fast)         | 150    | 6         | 0.07       | 0.9         | 0.9             | 10       | “Good enough” smaller model for speed. |



=== Best Tuned GBM (Grid Search): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 85.07859140163656
RMSE: 9.223805689715963
MAE: 5.0203285657367065
RMSLE: 0.377566035888705
Mean Residual Deviance: 85.07859140163656

=== Best Tuned GBM (Grid Search): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 91.10945400762134
RMSE: 9.54512723894351
MAE: 5.194156650961609
RMSLE: 0.3816283547466119
Mean Residual Deviance: 91.10945400762134

1. Baseline

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 114.35113479516781
RMSE: 10.693509002903014
MAE: 6.8773736310886955
RMSLE: NaN
Mean Residual Deviance: 114.35113479516781

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 130.7661342917119
RMSE: 11.43530210758386
MAE: 7.329962215328263
RMSLE: NaN
Mean Residual Deviance: 130.7661342917119

2. Shallow, many trees

ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 110.65503294120147
RMSE: 10.519269601127327
MAE: 6.682695503827137
RMSLE: NaN
Mean Residual Deviance: 110.65503294120147

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 126.56644504980714
RMSE: 11.25017533418067
MAE: 7.073546839945515
RMSLE: NaN
Mean Residual Deviance: 126.56644504980714

3. Deep, fewer trees

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 91.7882062585969
RMSE: 9.580616173221683
MAE: 5.328509737845169
RMSLE: NaN
Mean Residual Deviance: 91.7882062585969

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 95.98702883819506
RMSE: 9.797297016942737
MAE: 5.514143491953848
RMSLE: 0.40074237469548046
Mean Residual Deviance: 95.98702883819506

4. Deep, many trees

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 91.93809257471189
RMSE: 9.588435355922877
MAE: 5.231764815148115
RMSLE: NaN
Mean Residual Deviance: 91.93809257471189

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 93.48002688275191
RMSE: 9.66850696244006
MAE: 5.345524731556652
RMSLE: NaN
Mean Residual Deviance: 93.48002688275191

5. Very deep, stronger regular (BEST)

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 84.76604186830303
RMSE: 9.206847553223797
MAE: 4.98812160520563
RMSLE: 0.3815474224146949
Mean Residual Deviance: 84.76604186830303

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 90.93101501261914
RMSE: 9.535775532835236 (BEST results, before Standardized Features and Gaussianized Transformation)
MAE: 5.191092455933854
RMSLE: 0.3832104753363808
Mean Residual Deviance: 90.93101501261914

6. Small LR, many trees

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 90.4292185816191
RMSE: 9.509427878774785
MAE: 5.592691193791021
RMSLE: 0.428439634169597
Mean Residual Deviance: 90.4292185816191

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 97.79870034214616
RMSE: 9.889322542123205
MAE: 5.826224735140605
RMSLE: 0.42946848795981596
Mean Residual Deviance: 97.79870034214616

7. Higher LR, fewer trees

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 91.49473188605006
RMSE: 9.565287862163379
MAE: 5.644191763228555
RMSLE: NaN
Mean Residual Deviance: 91.49473188605006

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 98.5344615930079
RMSE: 9.926452618786225
MAE: 5.84946965348969
RMSLE: NaN
Mean Residual Deviance: 98.5344615930079

8. Row subsampling focus

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 88.72243456738013
RMSE: 9.419258705831375
MAE: 5.352213598185788
RMSLE: NaN
Mean Residual Deviance: 88.72243456738013

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 93.25689783158317
RMSE: 9.65696110749045
MAE: 5.504111165386398
RMSLE: 0.41372027970492276
Mean Residual Deviance: 93.25689783158317

9. Column subsampling focus

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 86.92616556434189
RMSE: 9.323420271785558
MAE: 5.222630328150416
RMSLE: 0.3998071394263899
Mean Residual Deviance: 86.92616556434189

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 93.66634787728104
RMSE: 9.678137624423464
MAE: 5.440744730538781
RMSLE: 0.395810289431508
Mean Residual Deviance: 93.66634787728104

10. Strong subsampling both

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 86.22803292746424
RMSE: 9.285905067760721
MAE: 5.170443614389118
RMSLE: NaN
Mean Residual Deviance: 86.22803292746424

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 91.52831535029598
RMSE: 9.567043187437589
MAE: 5.366661447314896
RMSLE: NaN
Mean Residual Deviance: 91.52831535029598

11. Regularized via min_rows

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 89.75114355328897
RMSE: 9.47370801499017
MAE: 5.508592454233012
RMSLE: NaN
Mean Residual Deviance: 89.75114355328897

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 96.85185283572888
RMSE: 9.84133389514495
MAE: 5.7233396949748165
RMSLE: 0.4223308596827402
Mean Residual Deviance: 96.85185283572888

12. Compact model (fast)

=== GBM Variant 1 (manual config): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 103.0828839281498
RMSE: 10.152974142001437
MAE: 6.3030748093834665
RMSLE: NaN
Mean Residual Deviance: 103.0828839281498

=== GBM Variant 1 (manual config): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 116.89347745165155
RMSE: 10.811728698577834
MAE: 6.703816220792203
RMSLE: NaN
Mean Residual Deviance: 116.89347745165155

13. Standarized Features

=== GBM (Standardized Features): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 85.34755139036145
RMSE: 9.238373849891627
MAE: 5.014173635886805
RMSLE: NaN
Mean Residual Deviance: 85.34755139036145

=== GBM (Standardized Features): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 90.3572327125182
RMSE: 9.505642151507608
MAE: 5.186564187530019
RMSLE: NaN
Mean Residual Deviance: 90.3572327125182

14. "Gaussianized" Features (Log Transform)

=== GBM (Gaussianized Features): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 85.13324971921752
RMSE: 9.226768108022306
MAE: 5.007806709648424
RMSLE: 0.3777978528924478
Mean Residual Deviance: 85.13324971921752

=== GBM (Gaussianized Features): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 90.59510933841406
RMSE: 9.518146318396983
MAE: 5.151140238331981
RMSLE: NaN
Mean Residual Deviance: 90.59510933841406

15. Dimensionality Reduction + GBM

## PCA-based GBM: project features, then train GBM on PCs

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Convert H2OFrames to pandas
train_pd = train.as_data_frame()
valid_pd = valid.as_data_frame()
test_pd  = test.as_data_frame()

y = "critical_temp"
X_cols = [c for c in train_pd.columns if c != y]

X_train_np = train_pd[X_cols].values
X_valid_np = valid_pd[X_cols].values
X_test_np  = test_pd[X_cols].values

y_train_np = train_pd[y].values
y_valid_np = valid_pd[y].values
y_test_np  = test_pd[y].values

# Standardize before PCA
scaler_pca = StandardScaler()
X_train_scaled = scaler_pca.fit_transform(X_train_np)
X_valid_scaled = scaler_pca.transform(X_valid_np)
X_test_scaled  = scaler_pca.transform(X_test_np)

# PCA: keep 95% variance
pca = PCA(n_components=0.95, random_state=176)
X_train_pca = pca.fit_transform(X_train_scaled)
X_valid_pca = pca.transform(X_valid_scaled)
X_test_pca  = pca.transform(X_test_scaled)

print("Original dim:", X_train_scaled.shape[1])
print("PCA dim:", X_train_pca.shape[1])

# Build pandas DataFrames for PCA features + target
pc_names = [f"PC{i+1}" for i in range(X_train_pca.shape[1])]

train_pca_pd = pd.DataFrame(X_train_pca, columns=pc_names)
valid_pca_pd = pd.DataFrame(X_valid_pca, columns=pc_names)
test_pca_pd  = pd.DataFrame(X_test_pca,  columns=pc_names)

train_pca_pd[y] = y_train_np
valid_pca_pd[y] = y_valid_np
test_pca_pd[y]  = y_test_np

# Convert to H2OFrames
train_pca = h2o.H2OFrame(train_pca_pd)
valid_pca = h2o.H2OFrame(valid_pca_pd)
test_pca  = h2o.H2OFrame(test_pca_pd)

# Train GBM on PCA features using your strong config
from h2o.estimators import H2OGradientBoostingEstimator

gbm_pca = H2OGradientBoostingEstimator(
    ntrees=800,
    max_depth=12,
    learn_rate=0.03,
    sample_rate=0.8,
    col_sample_rate=0.8,
    min_rows=10,
    stopping_rounds=5,
    stopping_metric="RMSE",
    stopping_tolerance=1e-4,
    seed=176
)

gbm_pca.train(
    x=pc_names,
    y=y,
    training_frame=train_pca,
    validation_frame=valid_pca
)

print("=== GBM PCA: Validation Performance ===")
perf_valid_pca = gbm_pca.model_performance(valid_pca)
perf_valid_pca.show()

print("\n=== GBM PCA: Test Performance ===")
perf_test_pca = gbm_pca.model_performance(test_pca)
perf_test_pca.show()

16. Feature Selection (Top-K by Mutual Information) + GBM

=== GBM Top-K: Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 0.017075552379526553
RMSE: 0.13067345705814382
MAE: 0.04201109492218169
RMSLE: 0.014284448715796453
Mean Residual Deviance: 0.017075552379526553

=== GBM Top-K: Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 0.01401085402685256
RMSE: 0.118367453410355
MAE: 0.03950371643147975
RMSLE: 0.006176284407183656
Mean Residual Deviance: 0.01401085402685256

17. Outlier Removal on Target + GBM

=== GBM (No Outliers): Validation Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 0.015961915027374297
RMSE: 0.12634047264188264
MAE: 0.03663568792197648
RMSLE: 0.010993928459842267
Mean Residual Deviance: 0.015961915027374297

=== GBM (No Outliers): Test Performance ===
ModelMetricsRegression: gbm
** Reported on test data. **

MSE: 0.040072989089466854
RMSE: 0.20018238955878925
MAE: 0.04001260170198679
RMSLE: 0.008050600615962404
Mean Residual Deviance: 0.040072989089466854

Summary from supercon_full_experiments.py

===== Building final summary table =====
                                         experiment  valid_RMSE  valid_MAE  test_RMSE  test_MAE  train_time_sec  ntrees  max_depth  learn_rate  sample_rate  col_sample_rate  min_rows
0                          GBM (No Target Outliers)    9.232673   5.010276   9.387624  5.124456       21.771128     800         12        0.03          0.8              0.8        10
1                       GBM (Standardized Features)    9.238374   5.014174   9.505642  5.186564       16.504978     800         12        0.03          0.8              0.8        10
2                       GBM (Gaussianized Features)    9.226768   5.007807   9.518146  5.151140       21.015670     800         12        0.03          0.8              0.8        10
3    GBM Manual 5: Very deep, strong regular (ID 5)    9.206848   4.988122   9.535776  5.191092       22.236318     800         12        0.03          0.8              0.8        10
4    GBM Manual 10: Strong subsampling both (ID 10)    9.285905   5.170444   9.567043  5.366661       10.992856     800          8        0.05          0.7              0.7         5
5        GBM Manual 8: Row subsampling focus (ID 8)    9.419259   5.352214   9.656961  5.504111        8.709814     500          8        0.05          0.7              1.0         5
6             GBM Manual 4: Deep, many trees (ID 4)    9.588435   5.231765   9.668507  5.345525       19.392201     800         10        0.03          1.0              1.0         5
7     GBM Manual 9: Column subsampling focus (ID 9)    9.323420   5.222630   9.678138  5.440745        8.144436     500          8        0.05          1.0              0.7         5
8            GBM Manual 3: Deep, fewer trees (ID 3)    9.580616   5.328510   9.797297  5.514143        9.015990     200         10        0.05          1.0              1.0         5
9   GBM Manual 11: Regularized via min_rows (ID 11)    9.473708   5.508592   9.841334  5.723340        7.251198     500          8        0.05          0.9              0.9        20
10        GBM Manual 6: Small LR, many trees (ID 6)    9.509428   5.592691   9.889323  5.826225       16.605163    1000          8        0.01          0.9              0.9         5
11      GBM Manual 7: Higher LR, fewer trees (ID 7)    9.565288   5.644192   9.926453  5.849470        3.690577     200          8        0.10          0.8              0.8        10
12                           GBM (PCA 95% variance)    9.761743   5.415984  10.073292  5.560451        9.374571     800         12        0.03          0.8              0.8        10
13                         GBM (Top-20 MI Features)   10.161546   5.938719  10.639558  6.207212        3.074209     800         12        0.03          0.8              0.8        10
14      GBM Manual 12: Compact model (fast) (ID 12)   10.152974   6.303075  10.811729  6.703816        1.779909     150          6        0.07          0.9              0.9        10
15         GBM Manual 2: Shallow, many trees (ID 2)   10.519270   6.682696  11.250175  7.073547        7.825186     500          4        0.05          1.0              1.0        10
16                              GBM Baseline (ID 1)   10.693509   6.877374  11.435302  7.329962        1.789592     100          5        0.10          1.0              1.0        10

Summary from run_superconductivity_models.py

===== Model Comparison Summary =====
         experiment_name        cv_RMSE   cv_MAE   test_RMSE   test_MAE   train_time_sec   ntrees   max_depth   learn_rate   sample_rate   col_sample_rate   min_rows   n_params
0        H2O GBM (Full)          9.4183   5.3780      8.8289     5.1251          40.7403    500.0        8.0        0.05          0.8              0.8         5.0      48823
1   H2O Random Forest             9.7518   5.6236      9.2194     5.3503         217.0345    500.0        20.0        N/A           0.8              0.8         5.0     963397
2      SVR (RBF + PCA 50)        14.9443   9.1151     14.4189     8.7580          22.2509      N/A         N/A        N/A            N/A               N/A         N/A     828240
3        H2O GLM (Linear)        17.8707  13.5830     17.6749    13.4082           2.6023      N/A         N/A        N/A            N/A               N/A         N/A         82

Summary from new run_superconductivity_models.py version

====================== MODEL COMPARISON (Sorted by Test RMSE) ======================
                        model_name              algorithm    cv_rmse  test_rmse     cv_mae   test_mae  n_params  n_trees  train_time_sec                                notes
0   GBM_VeryDeep_StrongRegular_ID5              GBM_sweep   9.387606   8.722747   5.194025   4.869640    154356    800.0       87.513724                                     
1                          H2O_GBM                    GBM   9.418285   8.828913   5.378000   5.125130     48823    500.0       40.762553                                     
2       GBM_StrongSubsampling_ID10              GBM_sweep   9.397730   8.829993   5.309198   5.069591     73956    800.0       54.683158                                     
3           GBM_ColSubsampling_ID9              GBM_sweep   9.437313   8.884047   5.375641   5.156845     47585    500.0       40.910616                                     
4           GBM_Deep_ManyTrees_ID4              GBM_sweep   9.556122   8.896479   5.319660   5.091122    104625    560.0       65.582003                                     
5          GBM_Deep_FewerTrees_ID3              GBM_sweep   9.630900   8.930458   5.452263   5.197162     45164    200.0       30.189330                                     
6           GBM_RowSubsampling_ID8              GBM_sweep   9.470983   8.958749   5.423870   5.207333     46174    500.0       43.114455                                     
7        GBM_HighLR_FewerTrees_ID7              GBM_sweep   9.668325   9.092212   5.670200   5.432023     15666    200.0       20.486852                                     
8             GBM_Reg_MinRows_ID11              GBM_sweep   9.628454   9.105844   5.621602   5.410557     30073    500.0       40.836224                                     
9        GBM_SmallLR_ManyTrees_ID6              GBM_sweep   9.612415   9.136206   5.700515   5.534838    117849   1000.0       82.172448                                     
10                          H2O_RF           RandomForest   9.751779   9.219407   5.623556   5.350259    963397    500.0      219.656855  very_large_model,very_slow_training
11           GBM_Compact_Fast_ID12              GBM_sweep  10.363667  10.107317   6.475930   6.413636      6240    150.0       14.035823                                     
12       GBM_Shallow_ManyTrees_ID2              GBM_sweep  10.787570  10.619564   6.854068   6.853285      7317    500.0       25.651416                                     
13                GBM_Baseline_ID1              GBM_sweep  10.931915  10.730998   7.044464   7.050939      2507    100.0        7.970552                                     
14                         SVR_PCA           SVR(RBF)+PCA  14.944278  14.418936   9.115104   8.758011    828240      NaN       25.169873     very_high_error,very_large_model
15                         H2O_GLM  LinearRegression(GLM)  17.870723  17.674896  13.582972  13.408205        82      NaN        3.135192                      very_high_error
====================================================================================

Notes about these results ^^:
- GLM performed the worse and ran into a "max iterations 31!" warning during training, which makes sense since it's a 
linear model looking for a solution in a non-linear problem.
- Random Forest had way too many features, almost a million, that's around 20x more than the base GBM model and it was 5x 
slower as well. This is a clear example that more trees doesn't necessarily mean better results. It most likely overfitted 
during training.
- SVR with PCA, performed worse than GBM and RF. Probably doesn't work very well with such high dimensionality from the dataset. 
Since our dataset contains over 21,000 samples and the complexity is quadratic, it reaches a point where there's too much data to be saved
in RAM. Overall, our dataset is so big SVR (RBF Kernel) struggles fitting the data cleanly. 

I will run ID5 "GBM_VeryDeep_StrongRegular" again and tweak Hyperparameters to see if I can get faster and better results.

===== Building final summary table (GBM base + sweep) =====
      experiment  valid_RMSE  valid_MAE  test_rmse  test_mae  train_time_sec  n_trees  max_depth  learn_rate  sample_rate  col_sample_rate  min_rows  n_params
0   GBM_n800_d11    9.353175   5.225667   8.701858  4.911562       72.801971      800       11.0        0.03          0.8              0.8        10    124306
1   GBM_n700_d11    9.357677   5.247294   8.711788  4.941400       67.290985      700       11.0        0.03          0.8              0.8        10    110638
2   GBM_n800_d10    9.349486   5.259132   8.714611  5.000508       66.681718      800       10.0        0.03          0.8              0.8        10    101444
3   GBM_n800_d12    9.387606   5.194025   8.722747  4.869640       82.262732      800       12.0        0.03          0.8              0.8        10    154356
4   GBM_n700_d12    9.384953   5.211309   8.726258  4.892069       74.699350      700       12.0        0.03          0.8              0.8        10    137016
5   GBM_n600_d12    9.387363   5.230574   8.735288  4.918195       69.105739      600       12.0        0.03          0.8              0.8        10    120514
6   GBM_n600_d11    9.365266   5.271230   8.738095  4.978409       64.128143      600       11.0        0.03          0.8              0.8        10     97743
7   GBM_n700_d10    9.367268   5.294229   8.741757  5.042474       62.718960      700       10.0        0.03          0.8              0.8        10     89892
8   GBM_n500_d12    9.396070   5.257332   8.762345  4.956132       64.666366      500       12.0        0.03          0.8              0.8        10    105485
9   GBM_n600_d10    9.388291   5.333194   8.769550  5.091805       57.653268      600       10.0        0.03          0.8              0.8        10     78877
10  GBM_n500_d11    9.382214   5.301326   8.780153  5.021293       57.616376      500       11.0        0.03          0.8              0.8        10     85800
11   GBM_n800_d9    9.390258   5.340266   8.805403  5.100737       59.456543      800        9.0        0.03          0.8              0.8        10     78949
12  GBM_n500_d10    9.416965   5.377632   8.825882  5.157239       51.011491      500       10.0        0.03          0.8              0.8        10     68390
13  H2O_GBM_Base    9.418285   5.378000   8.828913  5.125130       39.398304      500        8.0        0.05          0.8              0.8         5     48823
14   GBM_n700_d9    9.415660   5.384876   8.841752  5.159305       56.973970      700        9.0        0.03          0.8              0.8        10     69266
15   GBM_n600_d9    9.456359   5.440260   8.887069  5.218295       52.073334      600        9.0        0.03          0.8              0.8        10     60531
16   GBM_n500_d9    9.496477   5.499711   8.967154  5.298316       45.476267      500        9.0        0.03          0.8              0.8        10     52937
==========================================================

Notes ^^:
- GBM with 800 trees and depth of 11 seems to be the best model I've run so far compared to the other ones. I will try to experiment
with other Hyperparameters, manipulating dataset, and/or stopping early to see if we can get a more accurate model.

===== Building final summary table (GBM base + sweep) EARLY STOPPING =====
      experiment  valid_RMSE  valid_MAE  test_rmse  test_mae  train_time_sec  n_trees  max_depth  learn_rate  sample_rate  col_sample_rate  min_rows  n_params
0   GBM_n800_d11    9.372019   5.285202   8.757217  4.995664       55.421425      551       11.0        0.03          0.8              0.8        10     91738
1   GBM_n700_d11    9.372019   5.285202   8.757217  4.995664       58.238215      551       11.0        0.03          0.8              0.8        10     91738
2   GBM_n800_d10    9.375246   5.310766   8.757401  5.071927       56.114863      642       10.0        0.03          0.8              0.8        10     83374
3   GBM_n700_d10    9.376835   5.315238   8.763901  5.080841       57.453958      630       10.0        0.03          0.8              0.8        10     82011
4   GBM_n600_d11    9.374421   5.288776   8.764158  5.004190       57.446693      539       11.0        0.03          0.8              0.8        10     90264
5   GBM_n700_d12    9.388606   5.257996   8.766787  4.961228       59.810644      490       12.0        0.03          0.8              0.8        10    104046
6   GBM_n800_d12    9.388606   5.257996   8.766787  4.961228       58.084919      490       12.0        0.03          0.8              0.8        10    104046
7   GBM_n600_d12    9.390412   5.260478   8.771406  4.965062       62.436094      479       12.0        0.03          0.8              0.8        10    102413
8   GBM_n500_d12    9.395663   5.266390   8.778366  4.973413       60.075531      459       12.0        0.03          0.8              0.8        10     99221
9   GBM_n500_d11    9.382214   5.301326   8.780153  5.021293       56.523228      500       11.0        0.03          0.8              0.8        10     85800
10  GBM_n600_d10    9.390696   5.338634   8.783258  5.104414       54.993020      583       10.0        0.03          0.8              0.8        10     76862
11   GBM_n800_d9    9.397286   5.356023   8.821548  5.125503       57.644526      760        9.0        0.03          0.8              0.8        10     74999
12  GBM_n500_d10    9.416965   5.377632   8.825882  5.157239       51.186113      500       10.0        0.03          0.8              0.8        10     68390
13  H2O_GBM_Base    9.422826   5.401873   8.839749  5.157440       37.881779      458        8.0        0.05          0.8              0.8         5     45016
14   GBM_n700_d9    9.419321   5.393579   8.852707  5.170919       54.616793      680        9.0        0.03          0.8              0.8        10     67621
15   GBM_n600_d9    9.456359   5.440260   8.887069  5.218295       51.052400      600        9.0        0.03          0.8              0.8        10     60531
16   GBM_n500_d9    9.496477   5.499711   8.967154  5.298316       45.494767      500        9.0        0.03          0.8              0.8        10     52937
==========================================================

Notes from these results ^^:
- Despite stopping early the results still came back to be worse than when we let the model reach the max amount of trees.
