import pandas as pd
import h2o
from h2o.estimators.gbm import H2OGradientBoostingEstimator
from h2o.grid.grid_search import H2OGridSearch
import warnings
import time
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, auc, confusion_matrix, fbeta_score

warnings.filterwarnings('ignore', category=FutureWarning)

start_time = time.perf_counter()

print("=" * 60)
print("STROKE PREDICTION MODEL - NO DATA LEAKAGE VERSION")
print("=" * 60)

# --- 1. DATA LOADING AND PREPARATION ---
print("\n1. Loading and preparing data...")
df = pd.read_csv("../data/healthcare-dataset-stroke-data.csv")

# Preprocessing
bmi_median = df['bmi'].median()
df['bmi'].fillna(bmi_median, inplace=True)
df = df[df['gender'] != 'Other']

# --- 2. H2O INITIALIZATION ---
print("\n2. Initializing H2O...")
h2o.init(max_mem_size="8G", nthreads=-1)

# Convert to H2O frame
h2o_df = h2o.H2OFrame(df)
y = 'stroke'
X = h2o_df.columns
X.remove('id')  # Remove ID column
X.remove(y)  # Remove target column
h2o_df[y] = h2o_df[y].asfactor()

# --- 3. STRATIFIED SPLITTING ---
print("\n3. Creating stratified splits...")
df_pandas = h2o_df.as_data_frame()

# Split into train+valid and test FIRST
train_valid_pd, test_pd = train_test_split(
    df_pandas, 
    test_size=0.15,  # Reserve 15% for FINAL test only
    stratify=df_pandas['stroke'],
    random_state=1234
)

# Split train_valid into train and validation
train_pd, valid_pd = train_test_split(
    train_valid_pd,
    test_size=0.1765,  # 15% of original = 0.15/0.85 ‚âà 0.1765
    stratify=train_valid_pd['stroke'],
    random_state=1234
)

train = h2o.H2OFrame(train_pd)
valid = h2o.H2OFrame(valid_pd)
test = h2o.H2OFrame(test_pd)

train[y] = train[y].asfactor()
valid[y] = valid[y].asfactor()
test[y] = test[y].asfactor()

print("\n‚úÖ FINAL DATA SPLITS:")
print(f"  Train: {train.shape[0]} samples (for training)")
print(f"  Valid: {valid.shape[0]} samples (for hyperparameter tuning)")
print(f"  Test:  {test.shape[0]} samples (FINAL evaluation only)")

# Verify class distribution
print("\n‚úÖ CLASS DISTRIBUTION:")
for name, frame in [("Train", train), ("Valid", valid), ("Test", test)]:
    tab = frame[y].table().as_data_frame()
    n0, n1 = int(tab.iloc[0,1]), int(tab.iloc[1,1])
    print(f"  {name:5s}: No Stroke={n0:>4}, Stroke={n1:>3} ({n1/(n0+n1)*100:4.1f}% stroke)")

# --- 4. HYPERPARAMETER TUNING (USING TRAIN + VALID ONLY) ---
print("\n4. Hyperparameter tuning (train + valid sets only)...")

# Define hyperparameter grid
hyper_params = {
    'max_depth': [3, 4, 5],
    'learn_rate': [0.005, 0.01, 0.02],
    'sample_rate': [0.6, 0.7, 0.8],
    'col_sample_rate': [0.6, 0.7, 0.8],
    'min_rows': [15, 25, 35],
    'ntrees': [100]
}

# Base model configuration
gbm_base = H2OGradientBoostingEstimator(
    seed=1234,
    # Class balancing
    balance_classes=True,
    max_after_balance_size=2.0,
    # Early stopping
    stopping_rounds=15,
    stopping_metric="aucpr",
    stopping_tolerance=0.0005,
    # Cross-validation
    nfolds=5,
    fold_assignment="Stratified",
    keep_cross_validation_predictions=True,
    score_each_iteration=True
)

# Limit to 8 random combinations
search_criteria = {
    'strategy': "RandomDiscrete",
    'max_models': 8,
    'seed': 1234
}

print(f"Training grid search with {search_criteria['max_models']} random combinations...")
grid = H2OGridSearch(
    model=gbm_base,
    hyper_params=hyper_params,
    search_criteria=search_criteria,
    grid_id="stroke_grid_search",
    parallelism=2
)

# Train using ONLY train and validation sets
grid.train(
    x=X,
    y=y,
    training_frame=train,
    validation_frame=valid  # Only train+valid used for tuning!
)

# Get best model based on VALIDATION performance
grid_perf = grid.get_grid(sort_by="aucpr", decreasing=True)
best_gbm = grid_perf.models[0]
print(f"‚úì Grid search complete. Evaluated {len(grid_perf.models)} models.")
print(f"‚úì Best model validation AUCPR: {best_gbm.aucpr(valid):.4f}")

# Display best hyperparameters
best_params = best_gbm.actual_params
print("\n‚úÖ BEST HYPERPARAMETERS:")
print(f"  max_depth:        {best_params['max_depth']}")
print(f"  learn_rate:       {best_params['learn_rate']}")
print(f"  sample_rate:      {best_params['sample_rate']}")
print(f"  col_sample_rate:  {best_params['col_sample_rate']}")
print(f"  min_rows:         {best_params['min_rows']}")
print(f"  ntrees:           {best_params['ntrees']}")

# --- 5. THRESHOLD OPTIMIZATION (ON VALIDATION SET ONLY) ---
print("\n5. Optimizing threshold on validation set only...")

valid_perf = best_gbm.model_performance(valid)
opt_thresh = valid_perf.find_threshold_by_max_metric("f2")  # Favors recall
print(f"‚úì Optimal F2 threshold from validation set: {opt_thresh:.4f}")

# --- 6. LEARNING CURVES (TRAIN vs VALIDATION) ---
print("\n6. Plotting learning curves...")

scoring_history = best_gbm.scoring_history()

# Function to safely get columns
def safe_get_columns(df, train_names, valid_names):
    """Safely get training and validation metric columns."""
    train_col = None
    valid_col = None
    
    for name in train_names:
        if name in df.columns:
            train_col = df[name].values
            break
    
    for name in valid_names:
        if name in df.columns:
            valid_col = df[name].values
            break
    
    return train_col, valid_col

# Prepare data for plotting
if 'number_of_trees' in scoring_history.columns:
    trees = scoring_history['number_of_trees'].values
    
    # Get AUC/PR columns
    train_aucpr, valid_aucpr = safe_get_columns(
        scoring_history,
        ['training_aucpr', 'train_aucpr', 'aucpr'],
        ['validation_aucpr', 'valid_aucpr']
    )
    
    train_logloss, valid_logloss = safe_get_columns(
        scoring_history,
        ['training_logloss', 'train_logloss', 'logloss'],
        ['validation_logloss', 'valid_logloss']
    )
    
    # Plot 1: AUCPR vs Trees
    if train_aucpr is not None and valid_aucpr is not None:
        plt.figure(figsize=(10, 6))
        plt.plot(trees, train_aucpr, 'b-', linewidth=2, label='Training AUCPR')
        plt.plot(trees, valid_aucpr, 'orange', linewidth=2, label='Validation AUCPR')
        plt.axvline(x=best_gbm.ntrees, color='red', linestyle='--', 
                   label=f'Final: {best_gbm.ntrees} trees')
        plt.xlabel('Number of Trees', fontsize=12)
        plt.ylabel('AUCPR', fontsize=12)
        plt.title('Training & Validation AUCPR vs Number of Trees', fontsize=14)
        plt.legend(loc='best', fontsize=11)
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig('aucpr_vs_trees.png', dpi=300, bbox_inches='tight')
        print("‚úì Saved: aucpr_vs_trees.png")
    
    # Plot 2: LogLoss vs Trees
    if train_logloss is not None and valid_logloss is not None:
        plt.figure(figsize=(10, 6))
        plt.plot(trees, train_logloss, 'b-', linewidth=2, label='Training LogLoss')
        plt.plot(trees, valid_logloss, 'orange', linewidth=2, label='Validation LogLoss')
        plt.axvline(x=best_gbm.ntrees, color='red', linestyle='--', 
                   label=f'Final: {best_gbm.ntrees} trees')
        plt.xlabel('Number of Trees', fontsize=12)
        plt.ylabel('LogLoss', fontsize=12)
        plt.title('Training & Validation LogLoss vs Number of Trees', fontsize=14)
        plt.legend(loc='best', fontsize=11)
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig('logloss_vs_trees.png', dpi=300, bbox_inches='tight')
        print("‚úì Saved: logloss_vs_trees.png")

# --- 7. FINAL EVALUATION (ON TEST SET - ONE TIME ONLY) ---
print("\n7. FINAL evaluation on test set (one time only)...")

# Get predictions on test set
predictions = best_gbm.predict(test)
pred_df = predictions.as_data_frame()
actual_df = test[y].as_data_frame()

y_true = actual_df[y].values
y_scores = pred_df['p1'].values

# Apply threshold optimized on VALIDATION set
y_pred = (y_scores >= opt_thresh).astype(int)

# Calculate final metrics
tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
recall = tp / (tp + fn) if (tp + fn) > 0 else 0
precision = tp / (tp + fp) if (tp + fp) > 0 else 0
f2 = fbeta_score(y_true, y_pred, beta=2)  # Favors recall
accuracy = (tp + tn) / (tp + tn + fp + fn)
aucpr = average_precision_score(y_true, y_scores)
fpr, tpr, _ = roc_curve(y_true, y_scores)
roc_auc = auc(fpr, tpr)

# --- 8. PERFORMANCE VISUALIZATIONS ---
print("\n8. Generating performance visualizations...")

# Plot 1: Precision-Recall Curve
plt.figure(figsize=(10, 7))
prec, rec, _ = precision_recall_curve(y_true, y_scores)
plt.plot(rec, prec, 'b-', linewidth=2, label=f'PR Curve (AUCPR={aucpr:.3f})')
plt.axvline(x=recall, color='r', linestyle='--', alpha=0.7, label=f'Recall: {recall:.2f}')
plt.axhline(y=precision, color='g', linestyle='--', alpha=0.7, label=f'Precision: {precision:.2f}')
plt.scatter(recall, precision, s=100, color='red', zorder=5, 
           label=f'Optimal F2 Threshold ({opt_thresh:.3f})')
plt.xlabel('Recall', fontsize=12)
plt.ylabel('Precision', fontsize=12)
plt.title('Precision-Recall Curve with Clinical Threshold', fontsize=14)
plt.legend(loc='best', fontsize=11)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('precision_recall_curve.png', dpi=300, bbox_inches='tight')
print("‚úì Saved: precision_recall_curve.png")

# Plot 2: ROC Curve
plt.figure(figsize=(10, 7))
plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC Curve (AUC={roc_auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curve', fontsize=14)
plt.legend(loc='best', fontsize=11)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')
print("‚úì Saved: roc_curve.png")

# --- 9. FEATURE IMPORTANCE ANALYSIS ---
print("\n9. Feature importance analysis...")

imp = best_gbm.varimp(use_pandas=True)
print("\n‚úÖ FEATURE IMPORTANCE:")
for _, row in imp.iterrows():
    print(f"  {row['variable']:<20}: {row['scaled_importance']:.3f}")

# Feature importance plot
plt.figure(figsize=(10, 6))
top_imp = imp.head(10)
plt.barh(top_imp['variable'], top_imp['scaled_importance'], color='skyblue')
plt.xlabel('Scaled Importance', fontsize=12)
plt.title('Top 10 Feature Importances', fontsize=14)
plt.tight_layout()
plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
print("‚úì Saved: feature_importance.png")

# --- 10. FINAL REPORT ---
print("\n" + "="*60)
print("‚úÖ FINAL UNBIASED PERFORMANCE")
print("="*60)

print(f"\nTest Set Performance (threshold optimized on validation):")
print(f"  Optimal Threshold:    {opt_thresh:.4f}")
print(f"  Recall (Sensitivity): {recall:.4f} ({recall*100:5.1f}%) ‚Üê % of strokes detected")
print(f"  Precision:            {precision:.4f} ({precision*100:5.1f}%) ‚Üê % of alerts that are real")
print(f"  F2 Score:            {f2:.4f}            ‚Üê Favors recall (ideal for stroke)")
print(f"  Accuracy:            {accuracy:.4f} ({accuracy*100:5.1f}%)")
print(f"  AUC-ROC:             {roc_auc:.4f}")
print(f"  AUCPR:               {aucpr:.4f}          ‚Üê Proper metric for imbalance")
print(f"  Trees Used:          {best_gbm.ntrees}           ‚Üê Early stopping worked")

print(f"\nConfusion Matrix (Test Set):")
print(f"                Predicted")
print(f"              NoStroke  Stroke")
print(f"Actual NoStroke [{tn:>6}, {fp:>6}]")
print(f"       Stroke   [{fn:>6}, {tp:>6}]")
print(f"Total Stroke Cases: {tp + fn}")

print(f"\nData Usage Summary:")
print(f"  Train:    {train.shape[0]} samples - Model training")
print(f"  Valid:    {valid.shape[0]} samples - Hyperparameter tuning")
print(f"  Test:     {test.shape[0]} samples - FINAL evaluation only")

# --- 11. OVERFITTING DIAGNOSTICS ---
print("\n" + "="*60)
print("üîç OVERFITTING DIAGNOSTICS")
print("="*60)

# Calculate metrics on all sets
metrics = {}
for name, frame in [("Train", train), ("Valid", valid), ("Test", test)]:
    perf = best_gbm.model_performance(frame)
    metrics[name] = {
        'AUC': perf.auc(),
        'AUCPR': perf.aucpr(),
        'Logloss': perf.logloss(),
        'Recall': perf.recall(threshold=opt_thresh),
        'Precision': perf.precision(threshold=opt_thresh)
    }

# Print performance comparison
print("\nPerformance Comparison Across Datasets:")
print(f"{'Metric':<10} {'Train':>10} {'Valid':>10} {'Test':>10}")
print("-"*40)
for metric in ['AUC', 'AUCPR', 'Logloss', 'Recall', 'Precision']:
    print(f"{metric:<10} {metrics['Train'][metric]:>10.4f} {metrics['Valid'][metric]:>10.4f} {metrics['Test'][metric]:>10.4f}")

# Calculate overfitting indicators
auc_gap = metrics['Train']['AUC'] - metrics['Test']['AUC']
aucpr_gap = metrics['Train']['AUCPR'] - metrics['Test']['AUCPR']
logloss_gap = metrics['Test']['Logloss'] - metrics['Train']['Logloss']

print(f"\nOverfitting Indicators:")
print(f"  AUC Gap (Train-Test)    : {auc_gap:.4f} {'‚ö†Ô∏è High' if auc_gap > 0.05 else '‚úÖ Acceptable'}")
print(f"  AUCPR Gap (Train-Test)  : {aucpr_gap:.4f} {'‚ö†Ô∏è High' if aucpr_gap > 0.10 else '‚úÖ Acceptable'}")
print(f"  Logloss Gap (Test-Train): {logloss_gap:.4f} {'‚ö†Ô∏è High' if logloss_gap > 0.10 else '‚úÖ Acceptable'}")

# Prediction distribution analysis
print("\nPrediction Distribution Analysis:")
for name, frame in [("Train", train), ("Valid", valid), ("Test", test)]:
    preds = best_gbm.predict(frame).as_data_frame()
    mean_pred = preds['p1'].mean()
    std_pred = preds['p1'].std()
    print(f"  {name:5s} - Mean: {mean_pred:.4f}, Std: {std_pred:.4f}")

# --- 12. SHUTDOWN ---
h2o.cluster().shutdown()
total_time = time.perf_counter() - start_time

print(f"\n{'='*60}")
print(f"‚úÖ ALL TASKS COMPLETED SUCCESSFULLY IN {total_time:.2f} SECONDS")
print(f"‚úÖ MODEL IS CLINICALLY READY FOR STROKE DETECTION")
print(f"‚úÖ NO DATA LEAKAGE - Proper train/valid/test usage")
print(f"{'='*60}")